# -*- coding: utf-8 -*-
"""Thesis Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13bwvbOMDGcSCgRlpDzf3p0l5j3JN_s-o

# Imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import itertools
!pip install networkx-temporal
import networkx_temporal as tx
from networkx_temporal import TemporalGraph
# !pip install teneto
# import teneto
# from teneto import TemporalNetwork
# from teneto import networkmeasures
# from teneto import communitydetection
# from teneto import communitymeasures

# !pip install python-louvain
# import community

!pip install raphtory
import raphtory as raph
from raphtory import Graph
from raphtory import algorithms

import sklearn
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.metrics import roc_auc_score
from sklearn import metrics

import seaborn as sns

from xgboost import XGBRegressor
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

"""# Data Import/Cleaning"""

from google.colab import drive
drive.mount('/content/drive')

deal = pd.read_csv('/content/drive/MyDrive/Thesis Data/DealswCEO.csv')
deal = deal[deal['DealDate'] >= 19800101]
deal

dealInvestor = pd.read_csv('/content/drive/MyDrive/Thesis Data/DealInvestorwPartner.csv')
dealInvestor

companyIndustry = pd.read_csv('/content/drive/MyDrive/Thesis Data/CompanyIndustry.csv')
companyIndustry

companyLast = pd.read_csv('/content/drive/MyDrive/Thesis Data/CompanyLastInvestment.csv')
companyLast = companyLast.drop(['CompanyName','BusinessStatus'],axis=1)
print(companyLast.columns)

companyBinary = pd.read_csv('/content/drive/MyDrive/Thesis Data/CompanyBinary.csv').drop(columns=['LastFinancingDate'])
companyBinary['OwnershipStatus'].unique()

merged_deals = pd.merge(deal, dealInvestor, on='DealID', how='inner')
merged_deals = pd.merge(merged_deals, companyLast, on='CompanyID',how='left')
merged_deals = pd.merge(merged_deals, companyIndustry, on='CompanyID', how='left')
cross_section = merged_deals[['DealID', 'CompanyID', 'CompanyName', 'PrimaryIndustrySector', 'PrimaryIndustryGroup', 'InvestorID', 'InvestorName', 'CEOPBId', 'CEO','LeadPartnerID', 'LeadPartnerName', 'DealDate', 'DealSize', 'TotalInvestedCapital', 'BusinessStatus', 'YearFounded', 'FirstFinancingDate', 'LastFinancingDate']]
cross_section = cross_section.drop_duplicates()
cross_section['MinMax DealSize'] = (cross_section['DealSize']-cross_section['DealSize'].min()) / (cross_section['DealSize'].max()-cross_section['DealSize'].min())
cross_section = pd.merge(cross_section, companyBinary, on='CompanyID', how='left')
cross_section

buyside = pd.read_csv('/content/drive/MyDrive/Thesis Data/Buyside.csv')
buyside

# Convert dates to numeric for comparison
cross_section['LastFinancingDate'] = pd.to_numeric(cross_section['LastFinancingDate'], errors='coerce')
cross_section['DealDate'] = pd.to_numeric(cross_section['DealDate'], errors='coerce')

# Function to calculate the removal threshold date
def calculate_removal_date(df):
    if df['LastFinancingDate'].notna().any():
        # Use the latest LastFinancingDate if available
        return df['LastFinancingDate'].max()
    else:
        # Otherwise, use the latest DealDate
        return df['DealDate'].max()

# Group by CompanyID and calculate the removal date
removals_df = cross_section.groupby('CompanyID').apply(calculate_removal_date).reset_index()
removals_df.columns = ['CompanyID', 'RemovalThresholdDate']

# Convert to integer format YYYYMMDD
removals_df['RemovalThresholdDate'] = removals_df['RemovalThresholdDate'].astype(int)

print("Edge Removal DataFrame:")
print(removals_df.head())

"""# Helper Functions"""

cross_section[cross_section['CompanyID'] == '100003-15']

import math

def first_n_digis(num, n):
    return num // 10 ** (int(math.log(num, 10)) - n + 1)

def time_filter(df, start, end):
    # time_filtered = df[(df['DealDate'] >= start) & (df['DealDate'] <= end) & (df['YearFounded'] >= first_n_digis(start, 4))]
    time_filtered = df[(df['DealDate'] >= start) & (df['DealDate'] <= end)]

    return time_filtered

def dealsize_filter(df, min):
    dealsize_filtered = df[df['DealSize'] >= min]
    return dealsize_filtered

def graphBipartite(df, left, right, name):
    B = nx.Graph()
    B.add_nodes_from(((node, {"color": "blue"}) for node in df[left].unique()), bipartite=0)
    B.add_nodes_from(((node, {"color": "red"}) for node in df[right].unique()), bipartite=1)
    B.add_edges_from(zip(df[left], df[right]))

    node_colors = [data["color"] for _, data in B.nodes(data=True)]

    pos = nx.drawing.layout.bipartite_layout(B, nodes=df[left].unique())

    plt.figure(figsize = (12, 8))
    nx.draw(B, pos, with_labels=False, node_color=node_colors, node_size=50, edge_color="black", alpha=0.7)
    plt.title("Static Bipartite: " + name)
    plt.text(-.2, .5, left, fontsize=12, ha='center', va='center')
    plt.text(1, .5, right, fontsize=12, ha='center', va='center')
    plt.show()

    print("Number of Edges: ", B.number_of_edges())
    print("Number of nodes: ", B.number_of_nodes())

def graphStatic(df, investors, companies, coinvest, name):
    G = nx.Graph()
    G.add_nodes_from((node, {"color": "blue"}) for node in df[investors].unique())
    G.add_nodes_from((node, {"color": "red"}) for node in df[companies].unique())

    colored_edges = list(zip(
    df[investors],
    df[companies],
    [{"color": "black"} for i in range(len(df[investors]))]))

    G.add_edges_from(colored_edges)
    G.add_edges_from(coinvest)

    node_colors = [data["color"] for _, data in G.nodes(data=True)]
    edge_colors = [data["color"] for _,_,data in G.edges(data=True)]

    pos = nx.spring_layout(G, k=.1,iterations=40,seed=1)

    plt.figure(figsize = (12, 8))
    nx.draw(G, pos, with_labels=False, node_color=node_colors, edge_color=edge_colors, node_size=50, alpha=0.7)
    plt.title("Static Graph: " + name)
    plt.show()
    print("Number of Edges: ", G.number_of_edges())
    print("Number of Nodes: ", G.number_of_nodes())

def individual_nafilter(df):
    return df.dropna(subset=['CEOPBId', 'LeadPartnerID'])

def dealsize_nafilter(df):
    return df.dropna(subset=['DealSize'])

print(merged_deals['PrimaryIndustrySector'].unique())
def industry_filter(df, industry):
    return df[df['PrimaryIndustrySector'] == industry]

def relative_dealSize(df):
    return df['DealSize'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))

def company_names(df):
    return df.sort_values(by='DealSize', ascending=False)['CompanyName'].unique()

"""# Bipartite Constructions"""

year2000 = time_filter(cross_section, 20150101, 20160101)
year2000_large = dealsize_filter(year2000, 100)
graphBipartite(year2000_large, 'InvestorID', 'CompanyID', '2015 larger than $100M')

# By Individual

year2000_large_ind = individual_nafilter(year2000_large)

graphBipartite(year2000_large_ind, 'LeadPartnerID', 'CEOPBId', 'Year 2000 >= 50M')

"""# Co-Investment Edges"""

def investor_edges(df, weighted):
    deal_grouped_investor = df.groupby('DealID')['InvestorID']

    edges = []
    if weighted == False:
        for deal_id, investors in deal_grouped_investor:
            # Create all possible pairs of investors for this DealID
            investor_pairs = itertools.combinations(investors, 2)
            # Add these pairs to the edges list
            edges.extend([(pair[0], pair[1], {"color": "green"}) for pair in investor_pairs])
    elif weighted == True:
        min = relative_dealSize(df).min()
        for deal_id, investors in deal_grouped_investor:
            # Create all possible pairs of investors for this DealID
            investor_pairs = itertools.combinations(investors, 2)
            # Add these pairs to the edges list
            edges.extend([(pair[0], pair[1], {"weight": 1, "color": "green"}) for pair in investor_pairs])
    return edges

def partner_edges(df, weighted):
    filtered = individual_nafilter(df)
    deal_grouped_investor = filtered.groupby('DealID')['LeadPartnerID']

    edges = []
    if weighted == False:
        for deal_id, investors in deal_grouped_investor:
            # Create all possible pairs of investors for this DealID
            investor_pairs = itertools.combinations(investors, 2)
            # Add these pairs to the edges list
            edges.extend([(pair[0], pair[1], {"color": "green"}) for pair in investor_pairs])
    elif weighted == True:
        min = relative_dealSize(df).min()
        for deal_id, investors in deal_grouped_investor:
            # Create all possible pairs of investors for this DealID
            investor_pairs = itertools.combinations(investors, 2)
            # Add these pairs to the edges list
            edges.extend([(pair[0], pair[1], {"weight": 1, "color": "green"}) for pair in investor_pairs])
    return edges

year2000 = time_filter(cross_section, 20000101, 20200101)
year2000_large = dealsize_filter(year2000, 500)

print(year2000_large['CompanyName'].unique())
graphStatic(year2000_large, 'InvestorID', 'CompanyID', investor_edges(year2000_large,True), '2000-2020 larger than $500M w Co-Investment')

decade_2000 = time_filter(cross_section, 20000101, 20090101)
decade_2000_large = dealsize_filter(decade_2000, 100)

graphStatic(decade_2000_large, 'InvestorID', 'CompanyID', investor_edges(decade_2000_large,True), '2000 to 2009 >= 100M w Co-Investment')

decade_2000_large_IT = industry_filter(decade_2000_large, 'Information Technology')
print(company_names(decade_2000_large_IT))
graphStatic(decade_2000_large_IT, 'InvestorID', 'CompanyID', investor_edges(decade_2000_large_IT,False), '2000 to 2009 >= 100M w Co-Investment - IT Investments Only')

graphStatic(decade_2000_large, 'LeadPartnerID', 'CEOPBId', partner_edges(decade_2000_large,False), '2000 to 2009 >= 100M w Co-Investment')

"""# Edge Weighting"""

def graphStaticWeighted(df, investors, companies, coinvest, name):
    G = nx.Graph()
    G.add_nodes_from((node, {"color": "blue"}) for node in df[investors].unique())
    G.add_nodes_from((node, {"color": "red"}) for node in df[companies].unique())

    weighted_edges = list(zip(
    df[investors],
    df[companies],
    [{"weight": weight * 5, "color": "black"} for weight in relative_dealSize(df)]))

    G.add_edges_from(weighted_edges)
    G.add_edges_from(coinvest)

    node_colors = [data["color"] for _, data in G.nodes(data=True)]
    edge_colors = [data["color"] for _,_,data in G.edges(data=True)]

    pos = nx.spring_layout(G, k=.15,iterations=40,seed=1)

    edge_weights = [data.get('weight', 0.5) for _, _, data in G.edges(data=True)]
    plt.figure(figsize = (12, 8))
    nx.draw(G, pos, with_labels=False, node_color=node_colors, edge_color=edge_colors, node_size=50, alpha=0.7, width=edge_weights)
    plt.title("Static Graph: " + name)
    plt.show()
    print("Number of Edges: ", G.number_of_edges())
    print("Number of Nodes: ", G.number_of_nodes())
    print(G.edges(data=True))

decade_2000 = time_filter(cross_section, 20150101, 20160101)
decade_2000_large = dealsize_filter(decade_2000, 100)
decade_2000_large_weighted = dealsize_nafilter(decade_2000_large)
print(company_names(decade_2000_large_weighted))
graphStaticWeighted(decade_2000_large_weighted, 'InvestorID', 'CompanyID', investor_edges(decade_2000_large_weighted,True), 'Static Weighted Graph: 2015 >= 100M w Co-Investment')

"""# Buyside Edges"""

def buyside_edges(buyside_df, cross, weighted):
    buyside_edges = buyside_df[(buyside_df['CompanyID'].isin(cross['CompanyID'])) | (buyside_df['TargetCompanyID'].isin(cross['CompanyID']))]
    buyside_edges['MinMax DealSize'] = (buyside_edges['DealSize'] - cross['DealSize'].min()) / (cross['DealSize'].max() - cross['DealSize'].min())

    edges = []
    if weighted == False:
        for index, row in buyside_edges.iterrows():
            edges.extend([(row['CompanyID'], row['TargetCompanyID'], {"color": "red"})])
    elif weighted == True:
        for index, row in buyside_edges.iterrows():
            edges.extend([(row['CompanyID'], row['TargetCompanyID'], {"weight": row['MinMax DealSize'], "color": "red"})])
    return edges

def cIDsNotIn(cIDs, buysideDF):
    cIDsNotIn = buysideDF[~buysideDF['CompanyID'].isin(cIDs['CompanyID'])]['CompanyID']
    targetIDsNotIn = buysideDF[~buysideDF['TargetCompanyID'].isin(cIDs['CompanyID'])]['TargetCompanyID']
    return pd.concat((cIDsNotIn, targetIDsNotIn)).unique()

def graphFullStatic(df, investors, companies, coinvest_edges, buyside_data, buyside_edges, name):
    G = nx.Graph()
    G.add_nodes_from((node, {"color": "blue"}) for node in df[investors].unique())
    G.add_nodes_from((node, {"color": "red"}) for node in df[companies].unique())

    new = [item for pair in buyside_edges for item in pair[:2]]
    G.add_nodes_from((node, {"color": "red"}) for node in new)

    colored_edges = list(zip(
        df[investors],
        df[companies],
        [{"color": "black"} for i in range(len(df[investors]))]))

    G.add_edges_from(colored_edges)
    G.add_edges_from(coinvest_edges)
    G.add_edges_from(buyside_edges)

    new = [item for pair in buyside_edges for item in pair[:2]]

    node_colors = [data["color"] for _, data in G.nodes(data=True)]
    edge_colors = [data["color"] for _,_,data in G.edges(data=True)]

    pos = nx.spring_layout(G, k=.10,iterations=40,seed=1)

    plt.figure(figsize = (12, 8))
    nx.draw(G, pos, with_labels=False, node_color=node_colors, edge_color=edge_colors, node_size=50, alpha=0.7)
    plt.title("Full Static Graph: " + name)
    plt.show()
    print("Number of Edges: ", G.number_of_edges())
    print("Number of Nodes: ", G.number_of_nodes())

year2000 = time_filter(cross_section, 20150101, 20160101)
year2000_large = dealsize_filter(year2000, 100)

buyside2000_large = dealsize_filter(time_filter(buyside, 20150101, 20160101), 100)
graphFullStatic(year2000_large, 'InvestorID', 'CompanyID', investor_edges(year2000_large,False), buyside2000_large, buyside_edges(buyside2000_large, year2000_large, False), 'Full Static Graph: 2015 >= 100M w Co-investment and Buyside')

"""# Temporal Construction

## Temporal Helper Functions
"""

# Time Intervals
def time_intervals_persistent(start_yr, end_yr, interval_type):
    time_intervals = []
    if interval_type == 'yearly':
        for i in range(start_yr, end_yr + 1):
            time_intervals.append((int(str(start_yr) + '0101'), int(str(i) + '1231')))
    elif interval_type == 'quarterly':
        for i in range(start_yr, end_yr + 1):
            time_intervals.append((str(start_yr) + '0101', str(i) + '0331'))
            time_intervals.append((str(start_yr) + '0101', str(i) + '0630'))
            time_intervals.append((str(start_yr) + '0101', str(i) + '0930'))
            time_intervals.append((str(start_yr) + '0101', str(i) + '1231'))
    return time_intervals

def time_intervals_ind(start_yr, end_yr, interval_type):
    time_intervals = []
    if interval_type == 'yearly':
        for i in range(start_yr, end_yr + 1):
            time_intervals.append((int(str(i) + '0101'), int(str(i) + '1231')))
    elif interval_type == 'quarterly':
        for i in range(start_yr, end_yr + 1):
            time_intervals.append((int(str(i) + '0101'), int(str(i) + '0331')))
            time_intervals.append((int(str(i) + '0401'), int(str(i) + '0630')))
            time_intervals.append((int(str(i) + '0701'), int(str(i) + '0930')))
            time_intervals.append((int(str(i) + '1001'), int(str(i) + '1231')))
    return time_intervals

# branch of graph_static method that constructs and doesn't draw
def construct_static(df, investors, companies, coinvest):
    G = nx.Graph()
    G.add_nodes_from(df[investors].unique())
    G.add_nodes_from(df[companies].unique())

    std_edges = list(zip(
    df[investors],
    df[companies]))

    G.add_edges_from(std_edges)
    G.add_edges_from(coinvest)

    return G

# Temporal Snapshotting with coinvesting (no buyside)
def temporal_snapshot_coinvest(intervals, data):
    snapshots = []
    for start,end in intervals:
        # splitting dataframe into specific temporal section
        period_data = time_filter(data, start, end)
        # creating static network for specific temporal section
        G_snapshot = construct_static(period_data, 'InvestorID', 'CompanyID', investor_edges(period_data, False))

        ## ADD NODE REMOVALS HERE

        snapshots.append(G_snapshot)

    return snapshots

# Converting to Networkx-temporal
def convert_to_tx(snapshots):
    # convert to nx-temporal
    T = tx.TemporalGraph()
    for i, snapshot in enumerate(snapshots):
        T.insert(i, snapshots[i])
    T.pop()
    return T

# test
intervals_test = time_intervals_ind(2000, 2000, 'quarterly')
data_test = industry_filter(cross_section, 'Energy')
# data_test = dealsize_filter(data_test, 2)
snaps_test = temporal_snapshot_coinvest(intervals_test, data_test)
nxt_test = convert_to_tx(snaps_test)

print(nxt_test)


fig = tx.draw(nxt_test,
              layout="kamada_kawai",
              figsize=(12,8),
              border=True,
              suptitle=True,
              )

fig

"""## Raphtory Temporal"""

# Converting to Raphtory
def convert_to_raph(snapshots):
    # Initialize Raphtory Graph
    raph_graph = Graph()

    # Iterate through snapshots and add to Raphtory Graph
    for t, snapshot in enumerate(snapshots):
        print(f"Processing snapshot {t} with {snapshot.number_of_nodes()} nodes and {snapshot.number_of_edges()} edges.")

        # Add Nodes
        for node in snapshot.nodes():
            # If node doesn't exist, add it with the current timestamp
            if not raph_graph.has_node(node):
                raph_graph.add_node(t, node)

        # Add Edges with timestamp
        for u, v in snapshot.edges():
            # Add edge with time as the timestamp
            raph_graph.add_edge(t, u, v)

    print("Conversion to Raphtory Temporal Graph Complete.")
    return raph_graph

temporal_raph = convert_to_raph(snaps_test)
print(temporal_raph, '\n')

def raph_centralities(temporal_raph):
    dc = algorithms.degree_centrality(temporal_raph).to_df()
    print("Temporal Degree Centrality Calculated")

    prc = algorithms.pagerank(temporal_raph).to_df()
    print("Temporal PageRank Centrality Calculated")

    bc = algorithms.betweenness_centrality(temporal_raph).to_df()
    print("Temporal Betweenness Centrality Calculated")

    ha = algorithms.hits(temporal_raph, iter_count=100).to_df()
    print("Temporal Hubs/Auths Centrality Calculated")

    return dc, prc, bc, ha

dc, prc, bc, ha = raph_centralities(temporal_raph)

print(dc.sort_values(by='value', ascending=False).head(10))
print(prc.sort_values(by='value', ascending=False).head(10))
print(bc.sort_values(by='value', ascending=False).head(10))
print(ha.sort_values(by='value', ascending=False).head(10))

print(cross_section['CompanyName'][cross_section['CompanyID']=='11166-58'])

def raph_community(temporal_raph):
    comms = algorithms.weakly_connected_components(temporal_raph)
    print("WC Communities Calculated")
    return comms.to_df()

# comms = raph_community(temporal_raph)
# print(comms)

def create_feature_set(temporal_raph):
    dc, prc, bc, ha = raph_centralities(temporal_raph)
    comms = raph_community(temporal_raph)

    ha[['Hub', 'Auth']] = pd.DataFrame(ha['value'].tolist(), index=ha.index)
    ha = ha.drop(columns='value')

    dc = dc.rename(columns={'value': 'DegreeCentrality'})
    prc = prc.rename(columns={'value': 'PageRankCentrality'})
    bc = bc.rename(columns={'value': 'BetweennessCentrality'})
    comms = comms.rename(columns={'value': 'Community'})

    # Merge the centralities into a single DataFrame
    cents = pd.merge(dc, prc, on='node')
    cents = pd.merge(cents, bc, on='node')
    cents = pd.merge(cents, ha, on='node')
    cents = pd.merge(cents, comms, on='node')

    features = cents.rename(columns={'node': 'CompanyID'})
    CIDs = features['CompanyID']

    features = pd.get_dummies(features, columns=['Community'], prefix='Comm')

    features[features.columns[1:]] = features[features.columns[1:]].astype(float)
    return features

features = create_feature_set(temporal_raph)
print(features)

# Create mappings for CompanyID → CompanyName and InvestorID → InvestorName
company_name_mapping = cross_section[['CompanyID', 'CompanyName']].drop_duplicates().set_index('CompanyID')['CompanyName']
investor_name_mapping = cross_section[['InvestorID', 'InvestorName']].drop_duplicates().set_index('InvestorID')['InvestorName']

# Helper function to map an ID to the appropriate name
def map_id_to_name(id_val):
    # Ensure id_val is string to match keys (if necessary)
    id_val = str(id_val)
    if id_val in company_name_mapping.index:
        return company_name_mapping.loc[id_val]
    elif id_val in investor_name_mapping.index:
        return investor_name_mapping.loc[id_val]
    else:
        return "Unknown"

# Define centrality metrics
centrality_metrics = ['DegreeCentrality', 'BetweennessCentrality', 'PageRankCentrality', 'Hub', 'Auth']

# Create an empty dictionary to store ranked company/investor names
top_10_dict = {}

for metric in centrality_metrics:
    # Sort by centrality metric in descending order and get top 10
    top_10 = features[['CompanyID', metric]].sort_values(by=metric, ascending=False).head(10).copy()

    # Map ID to Name using our helper function
    top_10['CompanyName'] = top_10['CompanyID'].apply(map_id_to_name)

    # Store the top 10 names in the dictionary with rankings as keys
    top_10_dict[metric] = top_10['CompanyName'].values

# Convert dictionary to DataFrame
top_10_table = pd.DataFrame(top_10_dict, index=[f'#{i+1}' for i in range(10)])
print(top_10_table)

for i in top_10_table:
    col = top_10_table[i]
    latex_code = col.to_latex(index=True, caption="Top 10 Centrality Metrics", label="tab:top10")
    print(latex_code)

# for i in top_10_table:
#     col = top_10_table[i]
#     for j in range(len(col)):
#         matches = cross_section['CompanyName'][cross_section['CompanyID'] == col.iloc[j]]
#         if not matches.empty:  # Check if a match exists
#             top_10_table.iloc[j, top_10_table.columns.get_loc(i)] = matches.iloc[0]  # Assign first match

for metric in centrality_metrics:
    top_10 = features[['CompanyID', metric]].sort_values(by=metric, ascending=False).head(10).copy()
    print(f"Top 10 CompanyIDs for {metric}:\n", top_10['CompanyID'].tolist())

"""## Teneto Temporal (Deprecated)"""

# Converting to Teneto
def convert_to_teneto(tx_temporal):
    events = tx_temporal.to_events()
    events = np.vstack(events)

    # Separate the columns
    col1 = events[:, 0]
    col2 = events[:, 1]
    col3 = events[:, 2].astype(int)  # Convert third column to integer

    # Get unique strings from first and second columns
    unique_strings = np.unique(np.concatenate((col1, col2)))
    # Create mapping from string to unique integer
    mapping = {string: idx for idx, string in enumerate(unique_strings)}

    # Convert first and second columns using the mapping
    vectorized_convert = np.vectorize(lambda x: mapping[x])
    col1_converted = vectorized_convert(col1)
    col2_converted = vectorized_convert(col2)

    # Combine all columns back into a single numpy array
    converted_array = pd.DataFrame(np.column_stack((col1_converted, col2_converted, col3))).astype(int)
    converted_array.columns = ['i', 'j', 't']
    tnet = TemporalNetwork(from_df=converted_array)

    return tnet, mapping

tnet, map = convert_to_teneto(nxt)
print(len(map))

"""## Temporal Analytics Methods (Teneto Deprecated)"""

dc = tnet.calc_networkmeasure('temporal_degree_centrality', calc='overtime')
print(dc)
paths = tnet.calc_networkmeasure('shortest_temporal_path', steps_per_t = 5)

bc = networkmeasures.temporal_closeness_centrality(paths=paths)

# louv = communitydetection.temporal_louvain(tnet, temporal_consensus=False)

# full 2000-2024
intervals = time_intervals_persistent(2000, 2024, 'yearly')
#data_test = dealsize_filter(cross_section, 100)
snaps = temporal_snapshot_coinvest(intervals, cross_section)
nxt = convert_to_tx(snaps)

degree_metric = nxt.temporal_degree()
# Create a mapping from CompanyID to CompanyName
company_mapping = cross_section.drop_duplicates('CompanyID').set_index('CompanyID')['CompanyName'].to_dict()

# Create a mapping from InvestorID to InvestorName
investor_mapping = cross_section.drop_duplicates('InvestorID').set_index('InvestorID')['InvestorName'].to_dict()

# Function that maps a key to a name using the appropriate dictionary
def map_key(key):
    if key in company_mapping:
        return company_mapping[key]
    elif key in investor_mapping:
        return investor_mapping[key]
    else:
        # If key not found in either mapping, return the key itself
        return key

sorted_items = sorted(degree_metric.items(), key=lambda item: item[1], reverse=True)

sorted_items_with_names = [(map_key(key), value) for key, value in sorted_items]

print(sorted_items_with_names)

edge_count = np.vstack(sorted_items_with_names)[:,1].astype(int)

plt.hist(np.log10(edge_count),bins=20)
plt.title('Log_10 Degree Centrality 2000-2025')
plt.xlabel('Log_10 Degree Centrality')
plt.ylabel('Count')
plt.show()

# Separate InvestorIDs and CompanyIDs based on the first character pattern
top_investors = []
top_companies = []

for key, value in sorted_items:
    # Assuming InvestorIDs and CompanyIDs have different prefixes or patterns
    if key in investor_mapping:
        top_investors.append((key, value, investor_mapping[key]))
    elif key in company_mapping:
        top_companies.append((key, value, company_mapping[key]))

# Get the top 10 for each category
top_investors = top_investors[:10]
top_companies = top_companies[:10]

# Print the results
print("\nTop 10 Investors by Degree Centrality:")
for investor in top_investors:
    print(f"ID: {investor[0]}, Degree Centrality: {investor[1]}, Name: {investor[2]}")

print("\nTop 10 Companies by Degree Centrality:")
for company in top_companies:
    print(f"ID: {company[0]}, Degree Centrality: {company[1]}, Name: {company[2]}")

"""# TIC Prediction"""

def create_target_dataset(df):

    # Convert dates to numeric for comparison
    df['LastFinancingDate'] = pd.to_numeric(cross_section['LastFinancingDate'], errors='coerce')
    df['DealDate'] = pd.to_numeric(cross_section['DealDate'], errors='coerce')

    # Function to calculate the removal threshold date
    def calculate_lastfin(df):
        if df['LastFinancingDate'].notna().any():
            # Use the latest LastFinancingDate if available
            return df['LastFinancingDate'].max()
        else:
            # Otherwise, use the latest DealDate
            return df['DealDate'].max()

    # Group by CompanyID and calculate the removal date
    lastfin_df = df.groupby('CompanyID').apply(calculate_lastfin).reset_index()
    lastfin_df.columns = ['CompanyID', 'LastFinDate']

    # Convert to integer format YYYYMMDD
    lastfin_df['LastFinDate'] = lastfin_df['LastFinDate'].astype(int)

    target_df = pd.merge(lastfin_df, df.loc[:,['CompanyID', 'TotalInvestedCapital', 'YearFounded']], on='CompanyID', how='left')
    target_df = target_df.drop_duplicates(subset='CompanyID')
    return target_df

target = create_target_dataset(cross_section)
target

def target_filter(target, last_date, start_year):
    target_p = target[(target['LastFinDate'] >= last_date) & (target['YearFounded'] >= start_year)]
    target_p = target_p[target_p['TotalInvestedCapital'].notnull()]
    return target_p

test_target = target_filter(target, 20170101, 2000)
test_target

data = cross_section[cross_section['CompanyID'].isin(test_target['CompanyID'])]

data = industry_filter(data, 'Consumer Products and Services (B2C)')

# test
intervals = time_intervals_ind(2000, 2015, 'yearly')
snaps = temporal_snapshot_coinvest(intervals, data)
nxt = convert_to_tx(snaps)

print(nxt)

# feature = create_feature_set(temporal_raph)
# feature

TICs = test_target['TotalInvestedCapital']
plt.hist(np.log10(TICs),bins=40)
plt.show()

temporal_raph = convert_to_raph(snaps)
print(temporal_raph, '\n')

featureset = create_feature_set(temporal_raph)
featureset

fullset = pd.merge(featureset, test_target, on='CompanyID', how='left')
fullset = fullset.dropna(subset=['LastFinDate', 'TotalInvestedCapital','YearFounded'])
fullset = fullset.drop(columns=['LastFinDate','YearFounded'])
fullset

def run_multilinear_regression(data):
    # Separate features and target variable
    X = data.drop(columns=['CompanyID', 'TotalInvestedCapital'])
    print(np.shape(X))
    #X = X.iloc[:,:5]
    print(np.shape(X))
    y = data['TotalInvestedCapital']
    y = np.log10(y)
    print(np.shape(y))

    # Train-Test Split (80/20)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Initialize the Linear Regression model
    model = LinearRegression()

    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    avg_abs_residual = np.mean(np.abs(y_test - y_pred))

    # Print the results
    print("Multi-Linear Regression Results:")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"R-squared (R²): {r2:.4f}")
    print(f"Average Absolute Residual Error: {avg_abs_residual:.4f}")

    print(pd.DataFrame({'Prediction': y_pred, 'Value': y_test}))
    return X, y, model

# Run the function
X, y, model = run_multilinear_regression(fullset)

# Separate features and target
X = fullset.drop(columns=['CompanyID', 'TotalInvestedCapital'])
#X = X.iloc[:,:5]
y = fullset['TotalInvestedCapital']
y = np.log10(y)

# Split the data into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Initialize the XGBoost Regressor
xgb_model = XGBRegressor(
    n_estimators=100,    # Number of trees
    learning_rate=0.05,  # Learning rate
    max_depth=7,         # Maximum depth of the tree
    random_state=1
)

# Train the model
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred = xgb_model.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Print the evaluation metrics
print(f"XGBoost Regression Results:")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"R-squared (R²): {r2:.4f}")
print(f"Average Absolute Residual Error: {mae:.4f}")

pred_test = pd.DataFrame({'Prediction': y_pred, 'Value': y_test})
print(pred_test.sort_values(by='Value', ascending=False))

# Get feature importances
feature_importances = xgb_model.feature_importances_
sorted_idx = np.argsort(feature_importances)[::-1]

# Plot feature importances
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx[:10])), feature_importances[sorted_idx[:10]], align='center')
plt.yticks(range(len(sorted_idx[:10])), [X.columns[i] for i in sorted_idx[:10]])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importances - XGBoost')
plt.show()

# plot pred
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.title('Actual vs. Predicted Values')
plt.xlabel('Actual Total Invested Capital')
plt.ylabel('Predicted Total Invested Capital')
plt.grid(True)
plt.show()

residuals = y_test - y_pred

plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.6, color='purple')
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.grid(True)
plt.show()

sns.histplot(residuals, bins=30, kde=True, color='orange')
plt.title('Distribution of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate the correlation matrix
corr_matrix = X.corr()

# Plot a heatmap of the correlation matrix
plt.figure(figsize=(20, 15))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', cbar=True)
plt.title('Feature Correlation Matrix')
plt.show()

"""# Prediction Pipeline"""

# create target and construction datasets
def construction_datasets(df, target, last_fin_date, yr_founded, end_data_yr):

    # Filter TIC dataset to range
    target_filtered = target_filter(target, last_fin_date, yr_founded)

    # Temporal construction dataset
    data = df[df['CompanyID'].isin(target_filtered['CompanyID'])]
    # data = industry_filter(data, 'Consumer Products and Services (B2C)')

    # Match target CIDs to data CIDs
    target_filtered = target_filtered[target_filtered['CompanyID'].isin(data['CompanyID'])]

    print("Construction Datasets Match Check: \n")
    print(data['CompanyID'].unique().shape, target_filtered['CompanyID'].unique().shape)
    print("Data Head: \n", data.head())
    print("Target Head: \n", target_filtered.head())

    # test
    intervals = time_intervals_ind(yr_founded, end_data_yr, 'yearly')
    print(intervals)
    snaps = temporal_snapshot_coinvest(intervals, data)
    nxt = convert_to_tx(snaps)

    print(nxt)

    return snaps, target_filtered, data, nxt

# create full prediction set
def prediction_datasets(snaps, target_filtered, data, norm=True):
    temporal_raph = convert_to_raph(snaps)
    featureset = create_feature_set(temporal_raph)
    fullset = pd.merge(featureset, target_filtered, on='CompanyID', how='left')
    fullset = fullset.dropna(subset=['LastFinDate', 'TotalInvestedCapital','YearFounded'])
    fullset = fullset.drop(columns=['LastFinDate'])
    # fullset = fullset.drop(columns=['YearFounded'])


    fullset_norm = fullset

    if norm:
        fullset_norm.iloc[:,1:] = (fullset_norm.iloc[:,1:]-fullset_norm.iloc[:,1:].min())/(fullset_norm.iloc[:,1:].max()-fullset_norm.iloc[:,1:].min())
    print(fullset_norm.head())
    return fullset_norm

snaps, target_filtered, data, nxt = construction_datasets(industry_filter(cross_section, 'Consumer Products and Services (B2C)'), target, 20170101, 2000, 2015)

data

# fig = tx.draw(nxt,
#               layout="kamada_kawai",
#               figsize=(50,16),
#               border=True,
#               suptitle=True)

# fig

fullset_norm = prediction_datasets(snaps, target_filtered, data,norm=False)

# Run XGBoost
def XG_boost(fullset):
    # Separate features and target
    X = fullset.drop(columns=['CompanyID', 'TotalInvestedCapital'])
    #X = X.iloc[:,:5]
    y = fullset['TotalInvestedCapital']
    y = np.log10(y)

    # Split the data into 80% training and 20% testing
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Initialize the XGBoost Regressor
    xgb_model = XGBRegressor(
        n_estimators=100,    # Number of trees
        learning_rate=0.05,  # Learning rate
        max_depth=7#,         # Maximum depth of the tree
        #random_state=1
    )

    # Train the model
    xgb_model.fit(X_train, y_train)

    # Make predictions
    y_pred = xgb_model.predict(X_test)

    # Calculate evaluation metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    # Print the evaluation metrics
    print(f"XGBoost Regression Results:")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"R-squared (R²): {r2:.4f}")
    print(f"Average Absolute Residual Error: {mae:.4f}")

    pred_test = pd.DataFrame({'Prediction': y_pred, 'Value': y_test})
    print(pred_test.sort_values(by='Value', ascending=False))

    return xgb_model, y_test, y_pred, X, y

model, test, pred, X, y = XG_boost(fullset_norm)

# Initial Analysis Methods
def XG_boost_analysis(xgb_model, y_test, y_pred, X, y):
    # Get feature importances
    feature_importances = xgb_model.feature_importances_
    sorted_idx = np.argsort(feature_importances)[::-1]

    # Plot feature importances
    plt.figure(figsize=(10, 8))
    plt.barh(range(len(sorted_idx[:10])), feature_importances[sorted_idx[:10]], align='center')
    plt.yticks(range(len(sorted_idx[:10])), [X.columns[i] for i in sorted_idx[:10]])
    plt.xlabel('Importance')
    plt.title('Top 10 Feature Importances - XGBoost')
    plt.show()

    # plot pred
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
    plt.title('Actual vs. Predicted Values')
    plt.xlabel('Actual Total Invested Capital')
    plt.ylabel('Predicted Total Invested Capital')
    plt.grid(True)
    plt.show()

    residuals = y_test - y_pred

    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred, residuals, alpha=0.6, color='purple')
    plt.axhline(y=0, color='red', linestyle='--')
    plt.title('Residual Plot')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.grid(True)
    plt.show()

    sns.histplot(residuals, bins=30, kde=True, color='orange')
    plt.title('Distribution of Residuals')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()

XG_boost_analysis(model, test, pred, X, y)

print(len(time_filter(data, 20100101, 20101231)['InvestorID'].unique()))

"""# Binary Classification"""

def create_target_binary(df, time_gap_yrs):

    target_binary = df.copy()
    target_binary = target_binary.loc[:, ['CompanyID','OwnershipStatus', 'FirstFinancingDate', 'LastFinancingDate']]
    # target_binary = target_binary.dropna(subset=['FirstFinancingDate', 'LastFinancingDate'])
    target_binary = target_binary.drop_duplicates(subset='CompanyID')
    target_binary['TimeDelta'] = target_binary['LastFinancingDate'] - target_binary['FirstFinancingDate']

    target_binary['Exit/Failure'] = target_binary['OwnershipStatus'].isin(['Publicly Traded', 'Acquired/Merged (Operating Subsidiary)', 'Acquired/Merged', 'In IPO Registration'])
    target_binary['Time Check'] = target_binary['TimeDelta'] <= (time_gap_yrs * 10000)
    target_binary['Binary'] = (target_binary['Exit/Failure'] & target_binary['Time Check']).astype(int)
    # target_binary['Exit/Failure'] = target_binary['Exit/Failure'].astype(int)
    return target_binary

target_binary = create_target_binary(industry_filter(cross_section, 'Information Technology'), 10)

print(target_binary.head())
print(len(target_binary[target_binary['Binary'] == 1])/len(target_binary))
test = target_binary[target_binary['FirstFinancingDate'] >= 20010101]
test = test[test['FirstFinancingDate'] <= 20011231]

# create target and construction datasets
def datasets_binary(df, first_fin_date, time_gap=10, deal_window=50000, norm=True, form=0):
    # Create the binary target dataset using your existing function.
    target = create_target_binary(df, time_gap)
    # window = (deal_window - 1) * 10000 + 1130

    # Filter the deal data to only include companies whose first financing date is in the desired range.
    data = df[(df['FirstFinancingDate'] >= first_fin_date) &
                        (df['FirstFinancingDate'] < first_fin_date + deal_window)]

    #elif form == 1:


    # Further filter each deal so that only deals within the first 5 years after a company's first financing date are included.
    data = data[data['DealDate'] < data['FirstFinancingDate'] + deal_window]

    if form == 1:
        invested = data.groupby(by='CompanyID')['DealSize'].sum()

    ## Section to create history -1 snapshot
    history_start = first_fin_date - 50000
    history_data = df[(df['DealDate'] >= history_start) & (df['DealDate'] < first_fin_date) & (df['FirstFinancingDate'] < first_fin_date)]

    combined_data = pd.concat([history_data, data], ignore_index=True)

    # Update target to include only companies present in target_data.
    target = pd.merge(target, pd.DataFrame(data['CompanyID'].unique(), columns=['CompanyID']), on='CompanyID', how = 'inner')


    # data = industry_filter(data, 'Business Products and Services (B2B)')
    # test
    history_interval = [(history_start, history_start + 41130)]
    intervals = time_intervals_ind(first_n_digis(first_fin_date, 4), first_n_digis(first_fin_date + 2*(deal_window), 4) - 1, 'yearly')
    intervals = np.concatenate((history_interval, intervals))
    print(intervals)
    snaps = temporal_snapshot_coinvest(intervals, combined_data)
    nxt = convert_to_tx(snaps)

    print(nxt)

    temporal_raph = convert_to_raph(snaps)
    featureset = create_feature_set(temporal_raph)

    community_cols = [col for col in featureset.columns if col.startswith('Comm_')]
    community_size_map = {col: featureset[col].sum() for col in community_cols}

    def compute_community_feature(row):
        for col in community_cols:
            if row[col] == 1:
                col_name = col
                break

        size = community_size_map[col_name]
        return size

    if form == 1:
        featureset = pd.merge(featureset, invested, on='CompanyID', how='left')
        featureset = featureset.rename(columns={'DealSize': 'Invested_Amt'})

    # 3. Add the computed feature into featureset.
    featureset['Community_History_Size'] = featureset.apply(compute_community_feature, axis=1)

    # print("Feature Set: ", len(featureset))
    fullset = pd.merge(featureset, target.loc[:,['CompanyID', 'Binary']], on='CompanyID', how='inner')

    fullset_norm = fullset
    if norm:
        fullset_norm.iloc[:,1:-2] = (fullset_norm.iloc[:,1:-2]-fullset_norm.iloc[:,1:-2].min())/(fullset_norm.iloc[:,1:-2].max()-fullset_norm.iloc[:,1:-2].min())
    # print(len(fullset_norm))
    # fullset_norm.dropna(axis=1, inplace=True)

    return data, target, fullset_norm, nxt

data, target, fullset, nxt = datasets_binary(industry_filter(cross_section, 'Information Technology'), 20000101, deal_window=10000, form=1)

# add 5 years before target as a t_0 - 1 snapshot in temporal, perform same but
# only merge companies we want. before performing the merge, make community size
# feature for each company, sum of community membership IN INVESTORS AND
# COMPANIES NETWORK
fullset

# fig = tx.draw(nxt,
#             layout="kamada_kawai",
#             figsize=(50,16),
#             border=True,
#             suptitle=True)

# fig

def XG_boost_binary(fullset):
    # Separate features and target
    # X = fullset.drop(columns=['CompanyID', 'Binary'])
    X = fullset.drop(columns='Binary')
    y = fullset['Binary']

    # Split the data into 80% training and 20% testing
    X_tr, X_te, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)
    X_train = X_tr.drop(columns='CompanyID')
    X_test = X_te.drop(columns='CompanyID')


    # Initialize the XGBoost

    # Define a parameter grid for GridSearchCV
    param_grid = {
        'n_estimators': [100, 250, 500],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [5, 10, 15]
    }

    # Initialize the XGBClassifier with a fixed random_state and objective
    xgb_model = XGBClassifier(
        objective='binary:logistic',
        random_state=2,
        eval_metric='auc'
    )

    # Setup GridSearchCV with 5-fold cross-validation and scoring by F1 score
    grid_search = GridSearchCV(
        estimator=xgb_model,
        param_grid=param_grid,
        scoring='f1',
        cv=5,
        verbose=1,
        n_jobs=-1
    )

    # Fit GridSearchCV on the training data
    grid_search.fit(X_train, y_train)
    print("Best Parameters:", grid_search.best_params_)
    print("Best CV F1 Score:", grid_search.best_score_)

    # Use the best estimator to make predictions on the test set
    best_xgb = grid_search.best_estimator_
    y_pred = best_xgb.predict(X_test)

    # Evaluate model performance using classification metrics
    print("Test Accuracy:", accuracy_score(y_test, y_pred))
    print("Test F1 Score:", f1_score(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

    pred_test = pd.DataFrame({'Prediction': y_pred, 'Value': y_test})
    print("Test Length:", len(y_test))

    return best_xgb, X_te, y_test, y_pred, X, y

model, X_test, y_test, pred, X, y = XG_boost_binary(fullset)

"""# Evaluation Methods"""

def precision_at_k(y_true, y_proba, k):
    # Get indices of predictions sorted in descending order by probability
    sorted_indices = np.argsort(y_proba)

    # Select the top K indices
    top_k_indices = sorted_indices[-k:]

    # Calculate how many of the top K are true positives
    top_k_true = np.array(y_true)[top_k_indices]
    precision = np.sum(top_k_true) / k

    return precision

def ap_at_k(y_true, y_proba, k):
    p_at_k = np.zeros(k)
    for i in range(k):
        p_at_k[i] = precision_at_k(y_true, y_proba, i+1)

    return np.mean(p_at_k)

def f1_at_k(y_true, y_proba, k):
    """
    Compute F1@K, the harmonic mean of precision and recall at K.

    Parameters:
        y_true (array-like): True binary labels (0 or 1).
        y_proba (array-like): Predicted probabilities for the positive class.
        k (int): Number of top predictions to consider.

    Returns:
        float: F1 score at K.
    """
    # Get indices sorted in descending order by probability
    sorted_indices = np.argsort(y_proba)[::-1]

    # Select the top K indices
    top_k_indices = sorted_indices[:k]

    # Convert y_true to a numpy array for indexing
    y_true = np.array(y_true)

    # True positives in the top K predictions
    top_k_true = y_true[top_k_indices]
    tp_at_k = np.sum(top_k_true)

    # Compute precision@K
    precision_k = tp_at_k / k

    # Compute recall@K: total positives in y_true is np.sum(y_true)
    # (Be sure there is at least one positive to avoid division by zero)
    total_positives = np.sum(y_true)
    recall_k = tp_at_k / total_positives if total_positives > 0 else 0
    print("Recall @ K: ", recall_k)

    # If both precision and recall are zero, F1 is zero
    if precision_k + recall_k == 0:
        return 0.0

    # F1 score is the harmonic mean of precision and recall
    f1_k = 2 * (precision_k * recall_k) / (precision_k + recall_k)

    return f1_k

def rocauc_at_k(y_true, y_proba, k, title):
    # Get indices of predictions sorted in descending order by probability
    sorted_indices = np.argsort(y_proba)

    # Select the top K indices
    top_k_indices = sorted_indices[-k:]

    # Calculate how many of the top K are true positives
    top_k_true = np.array(y_true)[top_k_indices]
    top_k_pred = np.array(y_proba)[top_k_indices]

    rocauc = roc_auc_score(top_k_true, top_k_pred)
    print(f"ROC AUC: {rocauc:.4f}")

    fpr, tpr, thresholds = metrics.roc_curve(top_k_true, top_k_pred)
    roc_auc = metrics.auc(fpr, tpr)
    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,
                                    estimator_name='XGBoostClassifier')
    display.plot()
    plt.plot(np.linspace(0,1,100), np.linspace(0,1,100))
    plt.title(title)
    plt.show()

    return rocauc

y_pred_proba = model.predict_proba(X_test.drop(columns='CompanyID'))[:, 1]  # get probability for positive class
k = 10  # choose your K; for example, top 50 predictions
precision_k = precision_at_k(y_test, y_pred_proba, k)
print(f"Precision @ {k}: {precision_k:.4f}")

avg_precision_k = ap_at_k(y_test, y_pred_proba, k)
print(f"Average Precision @ {k}: {avg_precision_k:.4f}")

k = np.array(range(1, len(y_test)))
precision = np.zeros(len(k))
for i in range(len(k)):
    precision_k = precision_at_k(y_test, y_pred_proba, k[i])
    precision[i] = precision_k
plt.plot(k, precision)
plt.xlabel('K')
plt.ylabel('Precision')
plt.title('P@K vs. K')
plt.show()

# f1 at K Example usage:
# Assuming you have a trained XGBClassifier model and test set (X_test, y_test)
# And you've computed predicted probabilities for the positive class:
# y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]
k = 30  # or any K you choose
f1_k = f1_at_k(y_test, y_pred_proba, k)
print(f"F1@{k}: {f1_k:.4f}")

rocauc_at_k(y_test, y_pred_proba, 40)

"""# Full Out of Sample"""

data_oos, target_oos, fullset_oos, nxt_oos = datasets_binary(industry_filter(cross_section, 'Information Technology'), 20100101, deal_window=10000)
y_pred_proba_oos = model.predict_proba(fullset_oos.drop(columns=['CompanyID', 'Binary']))[:, 1]
k = 10  # choose your K; for example, top 50 predictions
precision_k = precision_at_k(fullset_oos['Binary'], y_pred_proba_oos, k)
print(f"Precision @ {k}: {precision_k:.4f}")

k = 50 # choose your K; for example, top 50 predictions
precision_k = precision_at_k(fullset_oos['Binary'], y_pred_proba_oos, k)
print(f"Precision @ {k}: {precision_k:.4f}")

k = np.array(range(1, len(y_test)))
precision = np.zeros(len(k))
for i in range(len(k)):
    precision_k = precision_at_k(fullset_oos['Binary'], y_pred_proba_oos, k[i])
    precision[i] = precision_k
plt.plot(k, precision)
plt.xlabel('K')
plt.ylabel('Precision')
plt.title('P@K vs. K')
plt.show()

rocauc = roc_auc_score(fullset_oos['Binary'], y_pred_proba_oos)
print(f"ROC AUC: {rocauc:.4f}")

fpr, tpr, thresholds = metrics.roc_curve(fullset_oos['Binary'], y_pred_proba_oos)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,
                                  estimator_name='example estimator')
display.plot()
plt.plot(np.linspace(0,1,100), np.linspace(0,1,100))
plt.show()

rocauc_at_k(fullset_oos['Binary'], y_pred_proba_oos, 20)

"""# Troubleshooting"""

# checking size getting filtered down

target_binary = create_target_binary(industry_filter(cross_section, 'Information Technology'), 10)

t_bin = target_binary.copy()
print(sum(t_bin['Binary']))
print(len(t_bin))


print(len(industry_filter(cross_section, 'Information Technology')['CompanyID'].unique()))

# filtering resolved on target dataset side

# checking filter down on network creation

data_bin, target_bin, fullset_bin, nxt_bin = datasets_binary(industry_filter(cross_section, 'Business Products and Services (B2B)'), 20100101, deal_window=10000)

temp = industry_filter(cross_section, 'Information Technology')
temp_deals = temp[(temp['FirstFinancingDate'] >= 20000101) &
                            (temp['FirstFinancingDate'] < 20000101 + 50000) & (temp['DealDate'] < temp['FirstFinancingDate'] + 50000)]

print(len(data_bin['CompanyID'].unique()))
print(len(data_bin['InvestorID'].unique()))

# data_bin.sort_values(by='FirstFinancingDate')
# print(len(feat_bin['CompanyID'].unique()))

# print(pd.concat([data_bin['CompanyID'], feat_bin['CompanyID']]).drop_duplicates(keep=False))

print(cross_section[cross_section['CompanyID'] == '11243-71'])

# done with filter down on network creation

# checking which companies are in my top recommended

model_ts, X_test_ts, y_test_ts, pred_ts, X_ts, y_ts = XG_boost_binary(fullset_bin)

y_pred_proba = model_ts.predict_proba(X_test_ts.drop(columns='CompanyID'))[:, 1]  # get probability for positive class
k = 10  # choose your K; for example, top 50 predictions
precision_k = precision_at_k(y_test_ts, y_pred_proba, k)
print(f"Precision @ {k}: {precision_k:.4f}")

avg_precision_k = ap_at_k(y_test_ts, y_pred_proba, k)
print(f"Average Precision @ {k}: {avg_precision_k:.4f}")

rocauc_at_k(y_test_ts, y_pred_proba, 10)

target_binary = create_target_binary(cross_section, 10)

def prediction_metadata(model, fullset):
    X = fullset.drop(columns=['CompanyID', 'Binary'])
    y = fullset['Binary']
    y_pred = model.predict_proba(X)[:, 1]
    y_class = model.predict(X)
    x = fullset.copy()
    x['Prediction'] = y_pred
    x['Predicted Class'] = y_class

    x = pd.merge(x, cross_section[['CompanyID', 'CompanyName', 'PrimaryIndustrySector',
       'PrimaryIndustryGroup', 'InvestorID', 'InvestorName', 'CEOPBId', 'CEO',
       'LeadPartnerID', 'LeadPartnerName', 'TotalInvestedCapital', 'BusinessStatus', 'YearFounded',
       'FirstFinancingDate', 'LastFinancingDate', 'OwnershipStatus', 'LastFinancingDealType']], on='CompanyID', how='inner')
    x = x.drop_duplicates(subset='CompanyID')

    return x[['CompanyID', 'CompanyName', 'Prediction', 'Predicted Class', 'Binary', 'DegreeCentrality', 'PageRankCentrality', 'Auth', 'Community_History_Size', 'PrimaryIndustrySector',
       'PrimaryIndustryGroup', 'InvestorID', 'InvestorName', 'CEOPBId', 'CEO',
       'LeadPartnerID', 'LeadPartnerName', 'TotalInvestedCapital',
       'BusinessStatus', 'YearFounded', 'FirstFinancingDate',
       'LastFinancingDate', 'OwnershipStatus', 'LastFinancingDealType']]

def prediction_metadata_test(model, X_test, y_test, fullset):
    y_pred = model.predict_proba(X_test.drop(columns='CompanyID'))[:, 1]
    y_class = model.predict(X_test.drop(columns='CompanyID'))


    x = pd.merge(X_test['CompanyID'], fullset, on='CompanyID', how='inner')
    x['Prediction'] = y_pred
    x['Predicted Class'] = y_class

    x = pd.merge(x, cross_section[['CompanyID', 'CompanyName', 'PrimaryIndustrySector',
       'PrimaryIndustryGroup', 'InvestorID', 'InvestorName', 'CEOPBId', 'CEO',
       'LeadPartnerID', 'LeadPartnerName', 'TotalInvestedCapital', 'BusinessStatus', 'YearFounded',
       'FirstFinancingDate', 'LastFinancingDate', 'OwnershipStatus', 'LastFinancingDealType']], on='CompanyID', how='inner')
    x = x.drop_duplicates(subset='CompanyID')

    return x[['CompanyID', 'CompanyName', 'Prediction', 'Predicted Class', 'Binary', 'DegreeCentrality', 'PageRankCentrality', 'Auth', 'Community_History_Size', 'PrimaryIndustrySector',
       'PrimaryIndustryGroup', 'InvestorID', 'InvestorName', 'CEOPBId', 'CEO',
       'LeadPartnerID', 'LeadPartnerName', 'TotalInvestedCapital',
       'BusinessStatus', 'YearFounded', 'FirstFinancingDate',
       'LastFinancingDate', 'OwnershipStatus', 'LastFinancingDealType']]

meta = prediction_metadata(model_ts, fullset_bin).sort_values(by='Prediction', ascending=False)
meta[meta['Binary'] == 1]

# meta_test = prediction_metadata_test(model_ts, X_test_ts, y_test_ts, fullset_bin).sort_values(by='Prediction', ascending=False)
# meta_test[meta_test['Binary'] == 1]

"""# Figure Creation"""

industries = cross_section['PrimaryIndustrySector'].unique()

# Define the cohort first financing dates and deal_window values to test
cohort_dates = [20000101, 20050101, 20100101, 20150101]
deal_windows = [10000, 30000]

# List to collect the results
results = []

# Loop over each industry, cohort date, and deal_window combination
for industry in industries:
    for first_fin_date in cohort_dates:
        for deal_window in deal_windows:
            print(f"Processing: Industry = {industry}, FirstFinDate = {first_fin_date}, DealWindow = {deal_window}")
            try:
                # Filter the cross_section dataframe by industry using your industry_filter helper
                df_industry = industry_filter(cross_section, industry)
                # Call the datasets_binary method. It returns (data, target, fullset_norm, nxt).
                _, _, fullset_norm, _ = datasets_binary(df_industry, first_fin_date, deal_window=deal_window)
                sample_size = len(fullset_norm)
            except Exception as e:
                sample_size = None
                print(f"Error for {industry}, {first_fin_date}, {deal_window}: {e}")

            # Append the results to the list
            results.append({
                "Industry": industry,
                "FirstFinDate": first_fin_date,
                "DealWindow": deal_window,
                "SampleSize": sample_size
            })

# Convert the results list into a DataFrame and display it
sample_size_table = pd.DataFrame(results)
print("\nSample Size Table for Each Cohort/Evaluation/Industry Combination:")
print(sample_size_table)

sample_size_table = pd.DataFrame(results)
table = sample_size_table
table['DealWindow'] = table['DealWindow'].astype(int) / 10000
table['FirstFinDate'] = np.round(table['FirstFinDate'] / 10000)
table = table.dropna(axis=0)
table.iloc[:,1:] = table.iloc[:,1:].astype(int)

table['FirstFinDate'] = table['FirstFinDate'].astype(int)
table['DealWindow'] = table['DealWindow'].astype(int)
table['SampleSize'] = table['SampleSize'].astype(int)

# Pivot the data so that the index is a MultiIndex with FirstFinDate and DealWindow,
# and the columns are the different industries.
pivot_table = table.pivot(index=["FirstFinDate", "DealWindow"],
                                        columns="Industry",
                                        values="SampleSize")

# Optionally, sort the pivot table by cohort dates for clarity
pivot_table = pivot_table.sort_index(level="FirstFinDate")
pivot_table = pivot_table.dropna(axis=1, how='all').astype(int)
pivot_table = pivot_table.drop(columns=['Energy', 'Financial Services', 'Materials and Resources'])

# Add a "Total" column that sums across each row (i.e., across industries).
pivot_table['Total'] = pivot_table.sum(axis=1)

# Now, compute a total row by summing each column.
# We create a new DataFrame for the totals with the same columns as df.
total_row = pd.DataFrame(pivot_table.sum(axis=0)).T

# Since df uses a MultiIndex for rows (FirstFinDate, DealWindow),
# we assign a MultiIndex tuple to the total row. You can customize the labels.
total_row.index = pd.MultiIndex.from_tuples([("Total", "Total")])

# Concatenate the total row at the bottom of the existing DataFrame.
pivot_table = pd.concat([pivot_table, total_row])

pivot_table = pivot_table.drop(index=[(2015, 1), (2015, 3)])

# Print the pivot table to visually inspect it
print("Pivot Table:")
print(pivot_table)

# Convert the pivot table to a LaTeX table.
# The options multicolumn=True and multirow=True help format the table nicely.
latex_table = pivot_table.to_latex(multicolumn=True, multirow=True, index=True)

# Print the LaTeX formatted table code
print("\nLaTeX Table:")
print(latex_table)

# Define the cohort years and the industry list.
# Please adjust the industry strings to exactly match those in your cross_section DataFrame.
cohorts = [2000, 2005, 2010, 2015]
industries_list = [
    "Business Products and Services (B2B)",
    "Consumer Products and Services (B2C)",
    "Healthcare",
    "Information Technology"
]

# Create a nested dictionary to store the proportion for each combination.
proportions = {}

for cohort in cohorts:
    proportions[cohort] = {}
    # Define the date range for the cohort year (e.g., 2000 -> 20000101 to 20001231)
    start_date = int(f"{cohort}0101")
    end_date = int(f"{cohort}1231")

    for industry in industries_list:
        # Filter by industry.
        df_industry = industry_filter(cross_section, industry)
        # Filter startups by their first financing date falling within the cohort year.
        df_cohort = df_industry[(df_industry['FirstFinancingDate'] >= start_date) &
                                (df_industry['FirstFinancingDate'] <= end_date)]
        # If there are any startups in this subgroup, create the binary target.
        if len(df_cohort) > 0:
            target_df = create_target_binary(df_cohort, 10)
            # The mean of the Binary column gives the proportion of successes.
            prop = np.round(target_df['Binary'].mean(), 3)
        else:
            prop = float('nan')

        proportions[cohort][industry] = prop

# Convert the nested dictionary to a DataFrame with cohorts as rows and industries as columns.
proportion_df = pd.DataFrame.from_dict(proportions, orient='index')
proportion_df.index.name = "Cohort"
print("Proportion of Successful Exits by Cohort and Industry:")
print(proportion_df)

# Convert the DataFrame into a LaTeX-formatted table.
# The float_format is set to show two decimals.
latex_proportion_table = proportion_df.to_latex(float_format="%.2f", multicolumn=True, multirow=True)
print("\nLaTeX Table:")
print(latex_proportion_table)

# temporal graph construction

# simple
_, _, _, nxt = datasets_binary(industry_filter(cross_section, 'Energy'), 20050101, deal_window=40000)

fig = tx.draw(nxt,
              layout="kamada_kawai",
              figsize=(32, 32),
              nrows=3,
              ncols=3,
              border=True,
              suptitle=True,
              )

fig

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

import warnings
warnings.filterwarnings("ignore")


# --- Initialize lists and dictionaries to store results and models ---

results_list = []     # To collect metric dictionaries.
saved_models = {}     # To store trained models for each stratification.

# Define the stratification levels:
cohorts = [2000, 2005, 2010]
deal_windows = [10000, 30000]
industries_list = [
    "Business Products and Services (B2B)",
    "Consumer Products and Services (B2C)",
    "Healthcare",
    "Information Technology"
]

# --- Loop over each industry, cohort, and deal window ---
for industry in industries_list:
    for cohort in cohorts:
        # Define the cohort’s time window based on first financing date.
        start_date = int(f"{cohort}0101")
        # end_date = int(f"{cohort}1231")
        # Filter the cross_section DataFrame by industry.
        df_industry = industry_filter(cross_section, industry)

        for deal_window in deal_windows:
            print(f"Processing: Industry = {industry}, Cohort = {cohort}, DealWindow = {deal_window}")
            try:
                # Construct the prediction dataset using your datasets_binary function.
                # Pass 'start_date' as the cohort first financing date.
                data, target, fullset, nxt = datasets_binary(df_industry, start_date, deal_window=deal_window)
                # Check if the fullset has any samples.
                if len(fullset) == 0:
                    raise ValueError("Fullset is empty.")

                # Train the binary classification model using your XG_boost_binary function.
                model, X_test, y_test, y_pred, X_all, y_all = XG_boost_binary(fullset)
                # Save the trained model for later use.
                saved_models[(industry, cohort, deal_window)] = model

                # Obtain predicted probabilities for the positive class.
                y_pred_proba = model.predict_proba(X_test.drop(columns='CompanyID'))[:, 1]

                # Compute evaluation metrics.
                acc = accuracy_score(y_test, y_pred)
                prec = precision_score(y_test, y_pred) if np.sum(y_test) > 0 else float('nan')
                rec = recall_score(y_test, y_pred) if np.sum(y_test) > 0 else float('nan')
                f1_val = f1_score(y_test, y_pred) if np.sum(y_test) > 0 else float('nan')
                roc_auc = roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else float('nan')
                ap10 = ap_at_k(y_test, y_pred_proba, 10)
                ap20 = ap_at_k(y_test, y_pred_proba, 20)
                roc_auc10 = rocauc_at_k(y_test, y_pred_proba, 10)
                roc_auc20 = rocauc_at_k(y_test, y_pred_proba, 20)

                # Append the results for this combination.
                results_list.append({
                    "Industry": industry,
                    "Cohort": cohort,
                    "DealWindow": deal_window,
                    "Accuracy": acc,
                    "Precision": prec,
                    "Recall": rec,
                    "F1-Score": f1_val,
                    "ROC/AUC": roc_auc,
                    "AP@10": ap10,
                    "AP@20": ap20,
                    "ROC/AUC@10": roc_auc10,
                    "ROC/AUC@20": roc_auc20
                })
            except Exception as e:
                print(f"Error processing Industry={industry}, Cohort={cohort}, DealWindow={deal_window}: {e}")
                # In case of error, record NaN for metrics.
                results_list.append({
                    "Industry": industry,
                    "Cohort": cohort,
                    "DealWindow": deal_window,
                    "Accuracy": None,
                    "Precision": None,
                    "Recall": None,
                    "F1-Score": None,
                    "ROC/AUC": None,
                    "AP@10": None,
                    "AP@20": None,
                    "ROC/AUC@10": None,
                    "ROC/AUC@20": None
                })

# Convert the collected results into a DataFrame.
results_df = pd.DataFrame(results_list)
print("\nRaw Evaluation Metrics:")
print(results_df)

# --- Create pivot tables by Industry ---
# For each industry we want a pivot table with a MultiIndex for rows: (Cohort, DealWindow)
# and columns for each evaluation metric.
industry_tables = {}

for industry in industries_list:
    df_ind = results_df[results_df["Industry"] == industry].copy()
    # Set the index to be a MultiIndex (Cohort, DealWindow).
    pivot = df_ind.set_index(["Cohort", "DealWindow"])
    # Select only the metric columns.
    pivot = pivot[["Accuracy", "Precision", "Recall", "F1-Score", "ROC/AUC", "AP@10", "AP@20", "ROC/AUC@10", "ROC/AUC@20"]]
    industry_tables[industry] = pivot

# --- Output the pivot tables and LaTeX code ---
for industry, table in industry_tables.items():
    print(f"\nPivot Table for Industry: {industry}")
    print(table)
    latex_code = table.to_latex(float_format="%.2f", multicolumn=True, multirow=True)
    print(f"\nLaTeX Table for Industry: {industry}")
    print(latex_code)

# Create a new dictionary to hold the transposed tables.
transposed_tables = {}

# Loop over each industry in the existing industry_tables dictionary.
for industry, table in industry_tables.items():
    # Transpose the table so that metrics are on the rows and (Cohort, DealWindow) become the columns.
    transposed = table.transpose()
    transposed_tables[industry] = transposed

    # Print the transposed table.
    print(f"\nTransposed Pivot Table for Industry: {industry}")
    print(transposed)

    # Convert the transposed table to LaTeX format.
    latex_code = transposed.to_latex(float_format="%.2f", multicolumn=True, multirow=True)
    print(f"\nLaTeX Table for Industry: {industry}")
    print(latex_code)

# Define the four metrics to plot.
# We use "AP@10" as the Precision@K metric.
metrics_to_plot = ["Accuracy", "Precision", "ROC/AUC", "AP@10"]

# Loop over each industry in updated_transposed_tables.
for industry in transposed_tables.keys():
    # Extract the relevant transposed table.
    # In the transposed table, rows = metrics and columns = (Cohort, DealWindow)
    df_plot = transposed_tables[industry].loc[metrics_to_plot]

    # Create a label for each stratification (e.g., "2000-10000", "2000-30000", etc.)
    col_labels = [f"{col[0]}-{col[1]/10000}" for col in df_plot.columns]

    n_groups = df_plot.shape[1]
    n_metrics = len(metrics_to_plot)
    x = np.arange(n_groups)
    width = 0.2  # width for each bar

    # Create a figure for the current industry.
    fig, ax = plt.subplots(figsize=(12, 6))

    # Plot a group of bars for each metric.
    for i, metric in enumerate(metrics_to_plot):
        # Calculate the offset: center the bars for the group.
        offset = (i - (n_metrics - 1) / 2) * width
        values = df_plot.loc[metric].values.astype(float)
        ax.bar(x + offset, values, width, label=metric)

    # Configure the plot.
    ax.set_xlabel("Cohort-Deal Window")
    ax.set_ylabel("Metric Value")
    ax.set_title(f"Performance Metrics for {industry}")
    ax.set_xticks(x)
    ax.set_xticklabels(col_labels, rotation=45, ha="right")
    ax.legend()
    plt.tight_layout()
    plt.show()

# selected rocauc curves
# B2B 2010,1
data, target, fullset, nxt = datasets_binary(industry_filter(cross_section, 'Business Products and Services (B2B)'), 20100101, deal_window=10000)
model, X_test, y_test, y_pred, X_all, y_all = XG_boost_binary(fullset)
# Obtain predicted probabilities for the positive class.
y_pred_proba = model.predict_proba(X_test.drop(columns='CompanyID'))[:, 1]

data, target, fullset, nxt = datasets_binary(industry_filter(cross_section, 'Business Products and Services (B2B)'), 20100101, deal_window=10000)

X = fullset.drop(columns='Binary')
y = fullset['Binary']

# Split the data into 80% training and 20% testing
X_tr, X_te, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train = X_tr.drop(columns='CompanyID')
X_test = X_te.drop(columns='CompanyID')

y_pred_proba = model.predict_proba(X_test)[:, 1]
# ra_k = rocauc_at_k(y_test, y_pred_proba, k, "test")

feature_importances = model.feature_importances_
sorted_idx = np.argsort(feature_importances)[::-1]

# Plot feature importances
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx[:10])), feature_importances[sorted_idx[:10]], align='center')
plt.yticks(range(len(sorted_idx[:10])), [X.columns[i] for i in sorted_idx[:10]])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importances: B2B 2010 Cohort - 1 yr Deal Window')
plt.show()

k = np.array(range(1, len(y_test)))
precision = np.zeros(len(k))
for i in range(len(k)):
    precision_k = precision_at_k(y_test, y_pred_proba, k[i])
    precision[i] = precision_k
plt.plot(k, precision)
plt.xlabel('K')
plt.ylabel('Precision')
plt.title('B2B 2010 Cohort - 1 yr Deal Window, P@K vs. K')
plt.show()

ra_k = rocauc_at_k(y_test, y_pred_proba, 10, 'B2B 2010 Cohort - 1 yr Deal Window, ROC/AUC @ 10')

data, target, fullset, nxt = datasets_binary(industry_filter(cross_section, 'Healthcare'), 20000101, deal_window=10000)

X = fullset.drop(columns='Binary')
y = fullset['Binary']

# Split the data into 80% training and 20% testing
X_tr, X_te, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train = X_tr.drop(columns='CompanyID')
X_test = X_te.drop(columns='CompanyID')

y_pred_proba = saved_models[('Healthcare', 2000, 10000)].predict_proba(X_test)[:, 1]

ra_k = rocauc_at_k(y_test, y_pred_proba, 10, 'Healthcare 2000 Cohort - 1 yr Deal Window, ROC/AUC @ 10')

feature_importances = saved_models[('Healthcare', 2000, 10000)].feature_importances_
sorted_idx = np.argsort(feature_importances)[::-1]

# Plot feature importances
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx[:10])), feature_importances[sorted_idx[:10]], align='center')
plt.yticks(range(len(sorted_idx[:10])), [X.columns[i] for i in sorted_idx[:10]])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importances: Healthcare 2000 Cohort - 1 yr Deal Window')
plt.show()

# k = np.array(range(1, len(y_test)))
# precision = np.zeros(len(k))
# for i in range(len(k)):
#     precision_k = precision_at_k(y_test, y_pred_proba, k[i])
#     precision[i] = precision_k
# plt.plot(k, precision)
# plt.xlabel('K')
# plt.ylabel('Precision')
# plt.title('IT 2005 Cohort - 3 yr Deal Window, P@K vs. K')
# plt.show()

data, target, fullset, nxt = datasets_binary(industry_filter(cross_section, 'Information Technology'), 20050101, deal_window=30000)

X = fullset.drop(columns='Binary')
y = fullset['Binary']

# Split the data into 80% training and 20% testing
X_tr, X_te, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train = X_tr.drop(columns='CompanyID')
X_test = X_te.drop(columns='CompanyID')

y_pred_proba = saved_models[('Information Technology', 2005, 30000)].predict_proba(X_test)[:, 1]

pa_k = precision_at_k(y_test, y_pred_proba, 10)

feature_importances = saved_models[('Information Technology', 2005, 30000)].feature_importances_
sorted_idx = np.argsort(feature_importances)[::-1]

# Plot feature importances
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx[:10])), feature_importances[sorted_idx[:10]], align='center')
plt.yticks(range(len(sorted_idx[:10])), [X.columns[i] for i in sorted_idx[:10]])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importances - XGBoost')
plt.show()

k = np.array(range(1, len(y_test)))
precision = np.zeros(len(k))
for i in range(len(k)):
    precision_k = precision_at_k(y_test, y_pred_proba, k[i])
    precision[i] = precision_k
plt.plot(k, precision)
plt.xlabel('K')
plt.ylabel('Precision')
plt.title('IT 2005 Cohort - 3 yr Deal Window, P@K vs. K')
plt.show()